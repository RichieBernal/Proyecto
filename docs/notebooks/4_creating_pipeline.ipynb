{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup\n",
        "In this notebook section, we will import the libraries needed to run this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9wFRz2IbzBua"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Constants\n",
        "In a Jupyter Notebook, creating constant variables can be important for several reasons:\n",
        "\n",
        "* **Readability and Maintainability**: Using constant variables with meaningful names can improve the readability of your code. It makes it easier for others (or even yourself in the future) to understand the purpose of the values being used throughout the notebook.\n",
        "\n",
        "* **Code Consistency**: By defining constants, you ensure that specific values are consistently used across the notebook. If you need to change the value later, you only have to modify it in one place, reducing the risk of errors due to inconsistent values.\n",
        "\n",
        "* **Preventing Magic Numbers**: Magic numbers are hardcoded numeric values scattered throughout the code without any explanation or context. Using constants instead of magic numbers makes the code self-documenting and provides context for the values used.\n",
        "\n",
        "* **Flexibility**: If you need to change a value that is used in multiple places, having it defined as a constant allows you to change it once, and the change will automatically apply throughout the notebook.\n",
        "\n",
        "* **Easy Debugging**: When debugging the code, having constants allows you to quickly check the values being used in different parts of the notebook without having to search for where they are defined.\n",
        "\n",
        "* **Unit Testing**: If you plan to write unit tests for your code, using constants can make it easier to define test cases and assert expected results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "THpyIPqCzBue"
      },
      "outputs": [],
      "source": [
        "DATASETS_DIR = './data/'\n",
        "URL = 'C:/Users/rbernal/Documents/GitHub/Proyecto/FAE/data/data_fire.csv'\n",
        "RETRIEVED_DATA = 'data_fire.csv'\n",
        "\n",
        "\n",
        "SEED_SPLIT = 404\n",
        "TRAIN_DATA_FILE = DATASETS_DIR + 'train.csv'\n",
        "TEST_DATA_FILE  = DATASETS_DIR + 'test.csv'\n",
        "\n",
        "TARGET  = 'STATUS'\n",
        "FEATURES = ['SIZE','FUEL','DISTANCE','DESIBEL','AIRFLOW','FREQUENCY']\n",
        "CATEGORICAL_VARS = ['FUEL']\n",
        "NUMERICAL_VARS = ['SIZE','DISTANCE','DESIBEL','AIRFLOW','FREQUENCY']\n",
        "\n",
        "SEED_MODEL = 404\n",
        "\n",
        "SELECTED_FEATURES = ['SIZE',\n",
        "                     'FUEL', \n",
        "                     #'FUEL_gasoline',\n",
        "                     'FUEL_lpg', \n",
        "                     'FUEL_kerosene',\n",
        "                     'FUEL_thinner',\n",
        "                     'DISTANCE','DESIBEL','AIRFLOW','FREQUENCY']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Functions\n",
        "Writing functions will help us for several things, for example:\n",
        "* **Modularity**: Functions allow you to break down complex problems into smaller, manageable pieces. Each function can handle a specific task, making the code easier to understand, test, and maintain. This concept is known as \"modularity.\"\n",
        "\n",
        "* **Reusability**: Once you define a function, you can use it multiple times throughout your code or even in other projects. This promotes code reuse and saves time since you don't have to rewrite the same logic each time you need it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_data(self):\n",
        "\n",
        "# Loading data from specific path\n",
        "    data = pd.read_csv(url) \n",
        "\n",
        "    # Create directory if it does not exist\n",
        "    if not os.path.exists(self.DATASETS_DIR):\n",
        "            os.makedirs(self.DATASETS_DIR)\n",
        "            print(f\"Directory '{self.DATASETS_DIR}' created successfully.\")\n",
        "        else:\n",
        "            print(f\"Directory '{self.DATASETS_DIR}' already exists.\")\n",
        "\n",
        "    # Save data to CSV file\n",
        "    data.to_csv(self.DATASETS_DIR + self.RETRIEVED_DATA, index=False)\n",
        "\n",
        "    return f'Data stored in {self.DATASETS_DIR + self.RETRIEVED_DATA}'\n",
        "\n",
        "data_retrieval(URL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Custom Transformers\n",
        "Custom transformers are really important if we want to have high-quality code, able to be maintaned, changed and be reused by other pieces of code.\n",
        "\n",
        "The following code is the migration from [3_create_convenient_classes.ipynb](3_create_convenient_classes.ipynb) notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom scikit-learn transformer to perform one-hot encoding for categorical variables.\n",
        "\n",
        "    Parameters:\n",
        "        variables (list or str, optional): List of column names (variables) to perform one-hot encoding for.\n",
        "            If a single string is provided, it will be treated as a single variable. Default is None.\n",
        "\n",
        "    Attributes:\n",
        "        variables (list): List of column names (variables) to perform one-hot encoding for.\n",
        "        dummies (list): List of column names representing the one-hot encoded dummy variables.\n",
        "\n",
        "    Methods:\n",
        "        fit(X, y=None):\n",
        "            Calculates the one-hot encoded dummy variable columns for the specified categorical variables from the training data.\n",
        "            It returns the transformer instance itself.\n",
        "\n",
        "        transform(X):\n",
        "            Performs one-hot encoding for the specified categorical variables and returns the modified DataFrame.\n",
        "\n",
        "    Example usage:\n",
        "    ```\n",
        "    from sklearn.pipeline import Pipeline\n",
        "\n",
        "    # Instantiate the custom transformer\n",
        "    encoder = OneHotEncoder(variables=['category1', 'category2'])\n",
        "\n",
        "    # Define the pipeline with the custom transformer\n",
        "    pipeline = Pipeline([\n",
        "        ('encoder', encoder),\n",
        "        # Other pipeline steps...\n",
        "    ])\n",
        "\n",
        "    # Fit and transform the data using the pipeline\n",
        "    X_transformed = pipeline.fit_transform(X)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, variables=None):\n",
        "        \"\"\"\n",
        "        Initialize the OneHotEncoder transformer.\n",
        "\n",
        "        Parameters:\n",
        "            variables (list or str, optional): List of column names (variables) to perform one-hot encoding for.\n",
        "                If a single string is provided, it will be treated as a single variable. Default is None.\n",
        "        \"\"\"\n",
        "        self.variables = [variables] if not isinstance(variables, list) else variables\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Calculates the one-hot encoded dummy variable columns for the specified categorical variables from the training data.\n",
        "\n",
        "        Parameters:\n",
        "            X (pd.DataFrame): Input data to be transformed.\n",
        "\n",
        "        Returns:\n",
        "            self (OneHotEncoder): The transformer instance.\n",
        "        \"\"\"\n",
        "        self.dummies = pd.get_dummies(X[self.variables], drop_first=True).columns\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Performs one-hot encoding for the specified categorical variables and returns the modified DataFrame.\n",
        "\n",
        "        Parameters:\n",
        "            X (pd.DataFrame): Input data to be transformed.\n",
        "\n",
        "        Returns:\n",
        "            X_transformed (pd.DataFrame): Transformed DataFrame with one-hot encoded dummy variables for the specified categorical variables.\n",
        "        \"\"\"\n",
        "        X = X.copy()\n",
        "        X = pd.concat([X, pd.get_dummies(X[self.variables], drop_first=True)], axis=1)\n",
        "        X.drop(self.variables, axis=1)\n",
        "\n",
        "        # Adding missing dummies, if any\n",
        "        missing_dummies = [var for var in self.dummies if var not in X.columns]\n",
        "        if len(missing_dummies) != 0:\n",
        "            for col in missing_dummies:\n",
        "                X[col] = 0\n",
        "\n",
        "        return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom scikit-learn transformer to select specific features (columns) from a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "        feature_names (list or array-like): List of column names to select as features from the input DataFrame.\n",
        "\n",
        "    Methods:\n",
        "        fit(X, y=None):\n",
        "            Placeholder method that returns the transformer instance itself.\n",
        "\n",
        "        transform(X):\n",
        "            Selects and returns the specified features (columns) from the input DataFrame.\n",
        "\n",
        "    Example usage:\n",
        "    ```\n",
        "    from sklearn.pipeline import Pipeline\n",
        "\n",
        "    # Define the feature names to be selected\n",
        "    selected_features = ['feature1', 'feature2', 'feature3']\n",
        "\n",
        "    # Instantiate the custom transformer\n",
        "    feature_selector = FeatureSelector(feature_names=selected_features)\n",
        "\n",
        "    # Define the pipeline with the custom transformer\n",
        "    pipeline = Pipeline([\n",
        "        ('feature_selector', feature_selector),\n",
        "        # Other pipeline steps...\n",
        "    ])\n",
        "\n",
        "    # Fit and transform the data using the pipeline\n",
        "    X_transformed = pipeline.fit_transform(X)\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_names):\n",
        "        \"\"\"\n",
        "        Initialize the FeatureSelector transformer.\n",
        "\n",
        "        Parameters:\n",
        "            feature_names (list or array-like): List of column names to select as features from the input DataFrame.\n",
        "        \"\"\"\n",
        "        self.feature_names = feature_names\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Placeholder method that returns the transformer instance itself.\n",
        "\n",
        "        Parameters:\n",
        "            X (pd.DataFrame): Input data to be transformed.\n",
        "\n",
        "        Returns:\n",
        "            self (FeatureSelector): The transformer instance.\n",
        "        \"\"\"\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Selects and returns the specified features (columns) from the input DataFrame.\n",
        "\n",
        "        Parameters:\n",
        "            X (pd.DataFrame): Input data to be transformed.\n",
        "\n",
        "        Returns:\n",
        "            X_selected (pd.DataFrame): DataFrame containing only the specified features (columns).\n",
        "        \"\"\"\n",
        "        return X[self.feature_names]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OrderingFeatures(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom scikit-learn transformer to order features (columns) in the same order as they appeared in the training data.\n",
        "\n",
        "    Parameters:\n",
        "        None\n",
        "\n",
        "    Attributes:\n",
        "        ordered_features (pd.Index): Index of column names representing the order of features as they appeared in the training data.\n",
        "\n",
        "    Methods:\n",
        "        fit(X, y=None):\n",
        "            Records the order of features from the training data and returns the transformer instance itself.\n",
        "\n",
        "        transform(X):\n",
        "            Reorders the features in the same order as they appeared in the training data and returns the modified DataFrame.\n",
        "\n",
        "    Example usage:\n",
        "    ```\n",
        "    from sklearn.pipeline import Pipeline\n",
        "\n",
        "    # Instantiate the custom transformer\n",
        "    feature_orderer = OrderingFeatures()\n",
        "\n",
        "    # Define the pipeline with the custom transformer\n",
        "    pipeline = Pipeline([\n",
        "        ('feature_orderer', feature_orderer),\n",
        "        # Other pipeline steps...\n",
        "    ])\n",
        "\n",
        "    # Fit and transform the data using the pipeline\n",
        "    X_transformed = pipeline.fit_transform(X)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the OrderingFeatures transformer.\n",
        "\n",
        "        Parameters:\n",
        "            None\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Records the order of features from the training data.\n",
        "\n",
        "        Parameters:\n",
        "            X (pd.DataFrame): Input data to be transformed.\n",
        "\n",
        "        Returns:\n",
        "            self (OrderingFeatures): The transformer instance.\n",
        "        \"\"\"\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            self.ordered_features = X.columns\n",
        "            print(self.ordered_features)\n",
        "        elif isinstance(X, np.ndarray):\n",
        "            self.ordered_features = np.arange(X.shape[1])\n",
        "        else:\n",
        "            raise ValueError(\"Input X must be a pandas DataFrame or a numpy array.\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Reorders the features in the same order as they appeared in the training data.\n",
        "\n",
        "        Parameters:\n",
        "            X (pd.DataFrame): Input data to be transformed.\n",
        "\n",
        "        Returns:\n",
        "            X_transformed (pd.DataFrame): Transformed DataFrame with features ordered as they appeared in the training data.\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            # print(X[self.ordered_features])\n",
        "            # print(\"return df\")\n",
        "            DROP_COLS_AFTER = ['FUEL']\n",
        "            X[self.ordered_features]\n",
        "            X.drop(DROP_COLS_AFTER, axis=1, inplace=True)\n",
        "            return X\n",
        "        elif isinstance(X, np.ndarray):\n",
        "            # print(\"return np\")\n",
        "            return X[:, self.ordered_features]\n",
        "        else:\n",
        "            raise ValueError(\"Input X must be a pandas DataFrame or a numpy array.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline\n",
        "The code below is a scikit-learn pipeline called fae_pipeline, that is used for data preprocessing and modeling for a Acoustic Extinguisher Fire dataset classification task. Each step in the pipeline corresponds to a specific data transformation or modeling step.\n",
        "\n",
        "* **`OneHotEncoder`**: This is a custom transformer that performs one-hot encoding for categorical variables. It takes the CATEGORICAL_VARS as input, which represents a list of categorical column names to be one-hot encoded. It creates binary dummy variables for each category.\n",
        "\n",
        "* **`OrderingFeatures`**: This is a custom transformer that orders the features (columns) in the same order as they appeared in the training data. It ensures that the order of columns in the transformed dataset is consistent with the order in which the pipeline was trained.\n",
        "\n",
        "* **`MinMaxScaler`**: This step scales the numerical features to a specified range, typically between 0 and 1, using the Min-Max scaling technique.\n",
        "\n",
        "* **`LogisticRegression`**: This is the final modeling step in the pipeline. It fits a logistic regression model to the preprocessed dataset. The model is specified with hyperparameters C=0.0005, class_weight='balanced', and random_state=SEED_MODEL. The C parameter is the regularization strength, 'balanced' sets the class weights to be inversely proportional to the class frequencies to handle class imbalance, and random_state is used for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(DATASETS_DIR + RETRIEVED_DATA)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "                                                        df.drop(TARGET, axis=1),\n",
        "                                                        df[TARGET],\n",
        "                                                        test_size=0.2,\n",
        "                                                        random_state=404\n",
        "                                                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "NhGa7W08zBuh"
      },
      "outputs": [],
      "source": [
        "transformations_pipeline = Pipeline(\n",
        "                              [\n",
        "                                ('dummy_vars', OneHotEncoder(variables=self.CATEGORICAL_VARS)),\n",
        "                                ('feature_selector', FeatureSelector(self.SELECTED_FEATURES)),\n",
        "                                ('aligning_feats', OrderingFeatures()),\n",
        "                                ('scaling', MinMaxScaler()),\n",
        "                              ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = transformations_pipeline.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logistic_regression = LogisticRegression(C=0.0005, class_weight='balanced', random_state=SEED_MODEL)\n",
        "logistic_regression.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test = transformations_pipeline.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(262, 13)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = logistic_regression.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_pred = logistic_regression.predict(X_test)\n",
        "proba_pred = logistic_regression.predict_proba(X_test)[:,1]\n",
        "print(f'test roc-auc : {roc_auc_score(y_test, proba_pred)}')\n",
        "print(f'test accuracy: {accuracy_score(y_test, class_pred)}')\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Persisting the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['trained_models/logistic_regression_output.pkl']"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "TRAINED_MODEL_DIR = 'trained_models/'\n",
        "PIPELINE_NAME = 'logistic_regression'\n",
        "PIPELINE_SAVE_FILE = f'{PIPELINE_NAME}_output.pkl'\n",
        "\n",
        "# Save the model using joblib\n",
        "save_path = TRAINED_MODEL_DIR + PIPELINE_SAVE_FILE\n",
        "joblib.dump(logistic_regression, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Basic input validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "input_data = X_test.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Making predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1]\n"
          ]
        }
      ],
      "source": [
        "# Sample single input data in dictionary format\n",
        "single_input_data = {\n",
        "        \"SIZE\": 3,\n",
        "        \"FUEL_lpg\": 0,\n",
        "        \"FUEL_kerosene\": 1,\n",
        "        \"FUEL_thinner\": 0,\n",
        "        \"DISTANCE\": 100,\n",
        "        \"DESIBEL\": 104,\n",
        "        \"AIRFLOW\": 8.8,\n",
        "        \"FREQUENCY\": 45\n",
        "}\n",
        "# Convert the single input data to a DataFrame\n",
        "single_input_df = pd.DataFrame([single_input_data])\n",
        "\n",
        "# Preprocess the single input data using the transformations_pipeline\n",
        "preprocessed_single_input = transformations_pipeline.transform(single_input_df)\n",
        "\n",
        "# Load the model using joblib\n",
        "trained_model = joblib.load(save_path)\n",
        "\n",
        "# Predict the target value using the loaded model\n",
        "predicted_value = trained_model.predict(preprocessed_single_input)\n",
        "\n",
        "print(predicted_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extra\n",
        "Use this code to debug the Custom Transformer pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# # Define the debug_print function to print DataFrame or array\n",
        "# def debug_print(X):\n",
        "#     if isinstance(X, pd.DataFrame):\n",
        "#         print(X.head())  # Print the first few rows of the DataFrame\n",
        "#     elif isinstance(X, np.ndarray):\n",
        "#         print(X[:5])  # Print the first 5 rows of the array\n",
        "        \n",
        "\n",
        "# # Define the preprocessor for categorical variables\n",
        "# categorical_preprocessor = Pipeline([\n",
        "#     ('categorical_imputer', CategoricalImputer(variables=CATEGORICAL_VARS_WITH_NA)),\n",
        "#     ('rare_labels', RareLabelCategoricalEncoder(tol=0.05, variables=CATEGORICAL_VARS)),\n",
        "#     ('dummy_vars', OneHotEncoder(variables=CATEGORICAL_VARS))\n",
        "# ])\n",
        "\n",
        "# # Define the preprocessor for numerical variables\n",
        "# numerical_preprocessor = Pipeline([\n",
        "#     ('missing_indicator', MissingIndicator(variables=NUMERICAL_VARS)),\n",
        "#     # ('cabin_only_letter', ExtractLetters()),\n",
        "#     ('median_imputation', NumericalImputer(variables=NUMERICAL_VARS_WITH_NA)),\n",
        "#     ('scaling', MinMaxScaler())\n",
        "# ])\n",
        "\n",
        "# # Use ColumnTransformer to apply the different preprocessors to their respective columns\n",
        "# preprocessor = ColumnTransformer(\n",
        "#     transformers=[\n",
        "#         ('categorical', categorical_preprocessor, CATEGORICAL_VARS),\n",
        "#         ('numerical', numerical_preprocessor, NUMERICAL_VARS)\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# # Combine the preprocessor with the logistic regression model in the final pipeline\n",
        "# titanic_pipeline = Pipeline([\n",
        "#     ('preprocessor', preprocessor),\n",
        "#     ('aligning_feats', OrderingFeatures()),\n",
        "#     ('log_reg', LogisticRegression(C=0.0005, class_weight='balanced', random_state=SEED_MODEL))\n",
        "# ])\n",
        "\n",
        "# # Debug each output after transformation\n",
        "# X_train_transformed = titanic_pipeline['preprocessor'].fit_transform(X_train)\n",
        "# debug_print(X_train_transformed)\n",
        "\n",
        "# # Fit the model\n",
        "# titanic_pipeline['log_reg'].fit(X_train_transformed, y_train)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
